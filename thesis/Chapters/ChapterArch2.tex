\chapter{Generative Adversarial Networks}

Generative Adversarial Networks are pairs of competing networks; a generator and a discriminator contest each other in a zero-sum game. The goal of the generator is to fool the discriminator by generating content which cannot reliably be differentiated from real ground-truth content. The discriminator attempts to tell generated content apart from real data. Each network is trained as the other's loss function (or one thereof).

The main advantage of a \acl{GAN} is that the loss function is learned; it does not need to be defined and it can exceed the performance obtained with a conventional loss that is a limiting factor. The generator can learn to restore high-frequency details which cannot be determined from the noisy observation alone (and that are not necessarily visible in the ground-truth) instead of averaging the many possible solutions.

Moreover, data may be added without a matching pair in some scenarios. This does not work with \acl{cGAN} nor can it be combined with other loss functions (combining losses is often necessary to ensure that the result matches and that the details are not overdone), but it can be of great usefulness to further train an existing network to perform better on specialized tasks.

\section{Types of GANs}

By input:
\begin{itemize}
  \item \ac{GAN}
  A \ac{GAN} is the simplest approach introduced in \cite{gan}. A generator is called with a given input, then the discriminator trains on a mini-batch made of the generated data and another mini-batch containing ground-truth data.

A \ac{GAN} does not necessarily need clean-noisy pairs of images because it only receives one image at a time. However, this feature may lead the generator to learn a mapping which has the features required to trick the discriminator without being related to the input image. For this reason we often combine the discriminator loss with a conventional loss function such as SSIM or L1 (and these losses do require a pair of clean-noisy images). The GAN could still be used for semi-supervised learning by training with conventional losses and a paired dataset until satisfying performance is met, then training further with a non-paired dataset using the GAN only. This could be beneficial for specialized images which would be tricky to include in a paired dataset, such as those depicting moving subjects or human faces.

While it may seem counter-intuitive to train the discriminator with mini-batches of only one label at a time, this phenomenon may be explained with the use of Batch Normalization which keeps running statistics and normalizes the activations on the given batch, it could therefore perform better given low-variance input data. \cite{gantechniques}\cite{bn} % TODO Our data is fed as a mix of different ISO values and could explain why the generator does not perform as well with BN
  \item \ac{cGAN}
  Conditional \acsp{GANs} are GANs which take two inputs, the generated data and typically a label that describes it. An approach often used in image generation is to concatenate the corrupted image with the generated input and feed these image pairs as the input data rather than using a label. \cite{pix2pix}\cite{cyclegan}\cite{pix2pixhd}. The benefit of potentially semi-supervised learning is no longer present but the discriminator is built-in with a correlative loss and the network is less likely to require a conventional loss function.
\end{itemize}
CycleGAN\cite{cyclegan} is a popular approach which enforces cyclic consistency, that is, the network must be able to regenerate the corrupted observation back from the generated one. We believe this approach would not function well for the problem of image denoising because of the random nature of noise, whereas CycleGAN applications tend to have simple outlines as the corrupted observation.

By loss:
\begin{itemize}
\item \ac{LSGAN}
\item \ac{DCGAN}
\end{itemize}
\section{Chosen GAN}

cGAN and GAN
PatchGAN
PatchGAN  already work on small 128x128 crops (of which the discriminator typically only sees 112x112 because we do not discriminate against bad borders)
The generator is the same UNet architecture used in previous

TODO Move this to later discussion

LSGAN
\section{Method}
The discriminator is trained on the generated input as soon as the network is initialized, but the generator uses only conventional loss functions (namely \ac{SSIM} and $\ell 1$) until a set \ac{SSIM} threshold is met. From this point the loss function is a weighted combination of the discriminator's \ac{MSE} loss (0.75), the \ac{SSIM} index (0.20) ensures stability and image quality, and the $\ell 1$ loss (0.05) provides some smoothness to prevent the discriminator excessive high-frequency details caused by the discriminator. 

Balanced initialization is crucial as experimental work has shown that using the discriminator too soon results in a generator whose only focus is to trick the discriminator and the image quality suffers to the point where the generator loses and its gradient vanishes. Starting off with a well-trained generator on the other hand gives no chance to the discriminator whose best bet is to be forever indecisive.

The network is still highly susceptible to failure after initialization. Common issues are overconfidence (with eventual mode collapse where the network outputs the same result), vanishing gradients, and indecision, therefore a good balance must be maintained all along the training process. We provide the discriminator's \ac{MSE} loss with noisy labels (+/- 0.03 probabilities) to prevent overconfidence. There is typically a 0.33 training ratio between the discriminator and the generator, the imbalance ensures that the two networks are not constantly trying to overcome each other's latest update. This ratio is lowered to 0.1 when the discriminator gets too accurate (95\%), and it is increased to 0.9 when the discriminator has approximately 50\% or below accuracy. (50\% is the ideal target but it often indicates an indecisive discriminator.)
% should investigate: https://arxiv.org/abs/1803.07422 (Patch-Based Image Inpainting with Generative Adversarial Networks)

\begin{algorithm}
\caption{(c)GAN training procedure}
\label{alg:gantrain}
\begin{algorithmic}
\FORALL{clean\_batch, noisy\_batch $\in$ Dataset}
\STATE ADAM\_optimizer\_D.clear\_gradient()
\STATE ADAM\_optimizer\_G.clear\_gradient()
\STATE \COMMENT{Generate a denoised ("fake") images batch}
\STATE generated\_batch $\leftarrow$ network\_G(noisy\_batch)
\STATE \COMMENT{Train the discriminator}
\IF{network\_D.is\_conditional}
\STATE fake\_minibatch $\leftarrow$ concatenate(
\STATE \quad remove\_borders(noisy\_batch),
\STATE \quad remove\_borders(generated\_batch).detach\_from\_graph())
\STATE real\_minibatch $\leftarrow$ concatenate(
\STATE \quad remove\_borders(noisy\_batch),
\STATE \quad remove\_borders(clean\_batch))
\ELSE
\STATE fake\_minibatch $\leftarrow$ remove\_borders(generated\_batch).detach\_from\_graph()
\STATE real\_minibatch $\leftarrow$ remove\_borders(clean\_batch)
\ENDIF
\STATE predictions\_on\_real\_data $\leftarrow$ network\_D(real\_minibatch)
\STATE loss\_real\_D $\leftarrow$ \ac{MSE}(
\STATE \quad predictions\_on\_real\_data,
\STATE \quad generate\_noisy\_probabilities(\TRUE))
\STATE loss\_real\_D.backpropagate()
\STATE predictions\_on\_fake\_data $\leftarrow$ network\_D(fake\_minibatch)
\STATE loss\_fake\_D $\leftarrow$ \ac{MSE}(
\STATE \quad predictions\_on\_fake\_data,
\STATE \quad generate\_noisy\_probabilities(\FALSE))
\STATE loss\_fake\_D.backpropagate()
\IF{discriminator\_learns}
\STATE \COMMENT{Whether the discriminator learns depends on randomness and a ratio determined by its previous performance}
\STATE ADAM\_optimizer\_D.update\_weights()
\ENDIF
\STATE \COMMENT{Train the generator}
\STATE loss\_G\_GAN $\leftarrow$ \ac{MSE}(
\STATE \quad predictions\_on\_fake\_data,
\STATE \quad generate\_noisy\_probabilities(\TRUE))
\STATE loss\_G\_SSIM $\leftarrow$ \ac{SSIM}(
\STATE \quad remove\_borders(generated\_batch),
\STATE \quad remove\_borders(clean\_batch))
\STATE loss\_G\_$\ell 1\leftarrow \ell 1$(
\STATE \quad remove\_borders(generated\_batch),
\STATE \quad remove\_borders(clean\_batch))
\STATE loss\_G\_weighted $\leftarrow$ 
\STATE \quad loss\_G\_GAN $\times (1- $ weight\_loss\_SSIM $-$ weight\_loss\_$\ell 1)$
\STATE \quad $+$ loss\_G\_SSIM $\times$ weight\_loss\_SSIM
\STATE \quad $+$ loss\_G\_$\ell 1 \times$ weight\_loss\_$\ell 1$
\STATE loss\_G\_weighted.backpropagate()
\STATE ADAM\_optimizer\_G.update\_weights()
\ENDFOR
\end{algorithmic}
\end{algorithm}
