\chapter{Results}

This chapter describes our use of \ac{CNN} to denoise images from the \ac{NIND}. The first section describes our suggested methods to handle the \ac{NIND} dataset. The \ac{CNN} Denoiser section first describes the various conventional network configurations we have worked with along with the Experimental results. The \ac{GAN} Denoiser section then describes our work and the results associated with (c)\ac{GAN} networks which require more experimental work and fine-tuning. 

\section{Dataset usage}\label{sec:Dataset Usage}
The \ac{NIND} is cropped in advance to speed up loading times. A crop size of 128x128 pixels was found to work well for training and larger crops did not significantly affect performance. We found that the border is most often corrupted in a U-Net model. Some network architectures perform better when required to learn to model entire crops down to their border, but it typically takes a lot of resources to get to that point and the result often still shows a grid pattern when the crops are stitched back together. We use a ``useful crop size" that is 0.75 the size of the actual crop size for U-Net models and approximately 0.875 with other architectures so that only the central part of a crop is used for stitching as well as for computing the loss. A script that crops the dataset in such overlapping blocks is provided for this purpose.

An epoch consists of training the model on every crop of any ISO value for every scene, that is, a random ISO value is fed every time a crop is loaded, therefore it takes several epochs for the model to train on all of the available data. The ground-truth is also selected randomly when multiple ones are available and basic data augmentation (rotation and/or translation) is performed.

\section{CNN Denoiser}\label{sec:CNN Denoiser}
%\subsubsection{Models}
We initially trained a DnCNN \cite{dncnn} model on our data. This model attained satisfying performance when trained to model the latent clean image instead of modeling the noise. It was further improved by using a convolution filter size of 5x5 instead of 3x3. The second architecture tested was a \ac{RED-Net} \cite{rednet} with 22-layers and a filter size of 5x5. This model obtained very good performance, albeit with an impractical runtime and memory use. We settled on a U-Net \cite{unet} architecture which provides slightly better performance with significantly lower runtime and memory use.

%\subsection{Experimental results}

We compared the performance obtained with the following methods:

\begin{enumerate}
\itemsep0em 
  \item\label{exp:nindxt1} U-Net trained on \ac{NIND} (X-T1 subset):\newline
  This model encompasses the main part of our dataset.
  \item\label{exp:sidd} U-Net trained on \ac{SIDD} (320 provided image pairs):\newline
  Compare the performance obtained using our dataset with 320 images from the \ac{SIDD} (Smartphone Image Denoising Dataset) \cite{sidd} which were made available for the 2019 NTIRE denoising challenge.
  \item\label{exp:bm3d} \ac{BM3D} \cite{bm3d} ( \cite{bm3d-gpu} implementation) with $\sigma$ = \{5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 93, 95, 97, 99\}\footnote{\label{sigmanote}We test every $\sigma$ value mentioned in Methods \ref{exp:bm3d} and \ref{exp:nindart} and report the value which yields the highest \ac{SSIM} for each test image.}:\newline
  BM3D has been ubiquitously used as a reference in non-learning based image denoising.
\end{enumerate}
In addition to the aforementioned reference methods, we consider the following experiments:
\begin{enumerate}
\itemsep0em 
\setcounter{enumi}{3}
  \item\label{exp:nindall}   U-Net trained on \ac{NIND} (dataset composed of the union of \ac{X-T1} and \ac{C500D} training scenes; 89.5 \% and 10.5 \%, respectively):\newline
  When tested on a \ac{C500D} image, this method can be compared with the first reference method to determine whether training on images acquired with the test image sensor helps or not, thereby assessing the generalization capabilities of our reference model to different sensors.  In addition, it shows whether adding data from a different sensor negatively affects performance.
  \item\label{exp:nindsidd} U-Net trained on the union of \ac{NIND}:\ac{X-T1}, \ac{NIND}:\ac{C500D} scenes) and \ac{SIDD} (320 pairs); 20.5 \%, 2.4 \%, and 77.1 \% respectively:\newline
  This model shows the performance impact of adding a wildly different type of noise to the training data.
  \item\label{exp:nind6400} U-Net trained on \ac{NIND} (\ac{X-T1} subset, ISO6400 noise only instead of random ISO sample):\newline
  This experiment tests whether a model trained for blind denoising performs significantly worse than one trained for a specific ISO value.
  \item\label{exp:nindart} U-Net trained on \ac{NIND} (\ac{X-T1} subset) with artificial gaussian noise added to the ground-truth. $\sigma$ = \{[1,55], [1,60], [1,80], [1,95]\}\footnoteref{sigmanote}:\newline
  This experiment compares the performance obtained by a model trained on our real data to the widely applied approach of applying synthetic gaussian noise to clean images.
  \item\label{exp:nindmakenoise} U-Net trained to reconstruct the noise on \ac{NIND} (\ac{X-T1} subset):\newline
  We applied the residual learning strategy proposed in \cite{dncnn} by training a model that reconstructs the noise and subtracts it from the image.
  \item\label{exp:nindred} \ac{RED-Net} \cite{rednet} trained on \ac{NIND} (\ac{X-T1} subset):\newline
  This uses the same data as Method \ref{exp:nindxt1} with a different network architecture

\end{enumerate}

\begin{table*}[!htbp]
\centering
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|llllllllll}
\ul{ISO value}                      & \ul{ISO200}   & \ul{ISO250}   & \ul{ISO500}   & \ul{ISO2500}  & \ul{ISO4000}  & \ul{ISO6400}  & \ul{High ISO} \\ \hline
Number of images                     & 5              & 3              & 2              & 2              & 2              & 5              & 9              \\ \hline
Noisy                                & \textbf{1.000} & 0.907          & 0.853          & 0.784          & 0.687          & 0.578          & 0.311          \\ \hline
NIND:X-T1 (U-Net)                     & 0.949          & \textbf{0.929} & \textbf{0.920} & \textbf{0.912} & \textbf{0.900} & \textbf{0.893} & \textbf{0.851} \\ \hline
SIDD (U-Net)                          & 0.906          & 0.907          & 0.904          & 0.864          & 0.882          & 0.860          & 0.814          \\ \hline
BM3D                                 & 0.941          & \textbf{0.925} & 0.913          & 0.875          & 0.870          & 0.852          & 0.785          \\ \hline
\ac{NIND}:X-T1+C500D (U-Net)               & 0.949          & \textbf{0.929} & \textbf{0.920} & \textbf{0.912} & \textbf{0.899} & \textbf{0.893} & \textbf{0.851} \\ \hline
\ac{NIND}:X-T1+C500D + \ac{SIDD} (U-Net)        & 0.947          & \textbf{0.928} & \textbf{0.919} & \textbf{0.910} & \textbf{0.898} & \textbf{0.892} & \textbf{0.850} \\ \hline
NIND:X-T1 ISO6400 only (U-Net)        & 0.919          & 0.915          & 0.911          & \textbf{0.907} & \textbf{0.901} & \textbf{0.894} & 0.821          \\ \hline
Reconstruct noise on NIND:X-T1 (U-Net)      & 0.950          & \textbf{0.926} & 0.914          & 0.901          & 0.876          & 0.840          & 0.664          \\ \hline
Artificial noise on NIND:X-T1 (U-Net) & 0.963          & 0.920          & 0.899          & 0.880          & 0.810          & 0.769          & 0.531          \\ \hline
NIND:X-T1 (\ac{RED-Net})                   & 0.940          & 0.923          & 0.915          & \textbf{0.907} & 0.892          & \textbf{0.886} & 0.842          \\ \hline
\end{tabular}%
}\caption[Average \acs{SSIM} index on 5 \acs{NIND}:\acs{X-T1} denoised scenes]{Average \acs{SSIM} index on 5 \acs{NIND}:\acs{X-T1} denoised scenes (ursulines-building, stefantiek, CourtineDeVillersDebris, MuseeL-Bobo, ursulines-red). The best performing models (to within two significant digits) are marked in bold.}
\label{tableNINDXT1}
\end{table*}

\begin{table*}[!htbp]
\centering
\resizebox{1\linewidth}{!}{%
\begin{tabular}{l|lllllll}
\ul{ISO value}                            & \ul{ISO100}   & \ul{ISO200}   & \ul{ISO400}   & \ul{ISO800}   & \ul{ISO1600}  & \ul{ISO3200}  & \ul{High ISO} \\ \hline
{Noisy}                                & \textbf{1.000} & 0.814          & 0.754          & 0.660          & 0.550          & 0.401          & 0.172          \\ \hline
{NIND:X-T1 (U-Net)}                     & 0.911          & \textbf{0.901} & \textbf{0.898} & \textbf{0.896} & \textbf{0.893} & \textbf{0.887} & \textbf{0.868} \\ \hline
{SIDD (U-Net)}                          & 0.894         & 0.892           & 0.890          & 0.889          & 0.885  & 0.876          & 0.850          \\ \hline
{BM3D}                                 & 0.921          & 0.890          & 0.884          & 0.877          & 0.871          & 0.860          & 0.813          \\ \hline
{\ac{NIND}:\ac{X-T1}+\ac{C500D} (U-Net)}               & 0.911          & \textbf{0.901} & \textbf{0.899} & \textbf{0.896} & \textbf{0.894} & \textbf{0.888} & \textbf{0.872} \\ \hline
{\ac{NIND}:\ac{X-T1}+\ac{C500D} + \ac{SIDD} (U-Net)}        & 0.912          & \textbf{0.901} & \textbf{0.899} & \textbf{0.896} & \textbf{0.893} & \textbf{0.888} & \textbf{0.871} \\ \hline
{\ac{NIND}:\ac{X-T1} ISO6400 only (U-Net)}        & 0.899          & \textbf{0.897} & \textbf{0.896} & 0.894          & \textbf{0.892} & \textbf{0.886} & \textbf{0.865} \\ \hline
{Reconstruct noise on \ac{NIND}:\ac{X-T1} (U-Net)}      & 0.908          & 0.895          & 0.887          & 0.875          & 0.855          & 0.807          & 0.617          \\ \hline
{Artificial noise on \ac{NIND}:\ac{X-T1} (U-Net)} & 0.946          & 0.879          & 0.864          & 0.836          & 0.802          & 0.716          & 0.430          \\ \hline
\end{tabular}%
}\caption{\acs{SSIM} index on \acs{NIND}:\acs{C500D} denoised set MuseeL-Bobo-C500D}
\label{tableNINDXT1onBoboC500D}
\end{table*}

\begin{table*}[!htbp]
\centering
\resizebox{1\linewidth}{!}{%
\begin{tabular}{l|lllllll}
\ul{ISO value}                       & \ul{ISO100}   & \ul{ISO200}   & \ul{ISO400}   & \ul{ISO800}   & \ul{ISO1600}  & \ul{ISO3200}  & \ul{High ISO} \\ \hline
\ul{\# images}                       & 13             & 9              & 9              & 9              & 8              & 10             & 9              \\ \hline
\ul{Noisy}                           & \textbf{0.954} & 0.766          & 0.707          & 0.619          & 0.501          & 0.380          & 0.220          \\ \hline
\ul{NIND:X-T1 (U-Net)}                & 0.878          & \textbf{0.856} & \textbf{0.854} & \textbf{0.848} & \textbf{0.845} & \textbf{0.834} & \textbf{0.805} \\ \hline
\ul{SIDD (U-Net)}                     & 0.851         & 0.838          & 0.837          & 0.834          & 0.830          & 0.813          & 0.774          \\ \hline
\ul{BM3D}                            & 0.899          & 0.836          & 0.825          & 0.816          & 0.811          & 0.789          & 0.749          \\ \hline
\ul{NIND:X-T1 ISO6400 only (U-Net)}   & 0.857          & 0.847          & \textbf{0.847} & 0.845          & \textbf{0.843} & \textbf{0.834} & 0.802          \\ \hline
\ul{Reconstruct noise on NIND:X-T1 (U-Net)} & 0.878          & 0.845          & 0.835          & 0.814          & 0.785          & 0.725          & 0.604          \\ \hline
\end{tabular}%
}\caption{Average \acs{SSIM} index on 10 \acs{NIND}:\acs{C500D} scenes denoised with models trained on \acs{NIND}:\acs{X-T1} or with \acs{BM3D}}
\label{tableXT1onC500D}
\end{table*}

\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{gfx/graphs/boboc500d.pdf}
%\input{bobo-c500d.pgf}
\caption[Denoising performance on the MuseeL-Bobo-C500D set over increased ISO value]{Denoising performance of different methods on the MuseeL-Bobo-C500D set over increased ISO value (\acs{SSIM} values shown in Table \ref{tableNINDXT1onBoboC500D})}
\label{fig:boboc500d}
\end{figure}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=1\linewidth]{gfx/comp/stefantiek.jpg}
\caption[Denoising stefantiek (visual comparison)]{Denoising stefantiek. 1: ISO200 ground-truth (1/20 s), 2: high ISO (1/1900 s), 3: 2 denoised using U-Net model trained with \ac{NIND}, 4: 2 denoised using U-Net model trained with \ac{SIDD} (320-sets), 5: 2 denoised using \ac{BM3D} ($\sigma=90$\footnoteref{sigmanote})}
\label{fig:stefantiek}
\end{figure*}

% \begin{figure}[!htbp]
% \centering
% \includegraphics[width=1\linewidth]{../comp/NIND_MuseeL-Bobo-C500D.jpg}
% \caption{Denoising MuseeL-Bobo-C500D. 1: ISO100 ground-truth (4s), 2: ISO3200 (1/10s), 3: 2 denoised using U-Net model trained with \ac{NIND} (X-T1 subset), 4: 2 denoised using U-Net model trained with \ac{SIDD} (320-sets), 5: 2 denoised using BM3D ($\sigma=60$)}
% \label{fig:boboc500dvis}
% \end{figure}

\begin{figure*}[!htbp]
\centering
\includegraphics[width=1\linewidth]{gfx/comp/txt.jpg}
\caption[Denoising text present on MuseeL-Sepik-C500D (visual comparison)]{Denoising text present on MuseeL-Sepik-C500D. 1: ISO200 ground-truth (2 s), 2: high ISO (1/30 s), 3: 2 denoised using U-Net model trained with \acs{NIND}:\acs{X-T1}, 4: 2 denoised using U-Net model trained with \ac{SIDD} (320-sets), 5: 2 denoised using \acs{BM3D} ($\sigma=99$\footnoteref{sigmanote})}
\label{fig:text}
\end{figure*}

\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{gfx/comp/comp-noisy-nind-bm3d30.jpg}
\caption[Denoising of an unpaired "high-speed" image (visual comparison)]{Comparison between a noisy ISO6400 crop (top), one denoised with a model trained on \ac{NIND} (middle), and one on which \ac{BM3D} ($\sigma=30$) has been applied (bottom). Our model appears to perform well on dynamic scenes despite having been trained on static scenes.}
\label{fig:visualpigeons}
\end{figure}


Each network is trained for 48-hours on a GeForce GTX 1080 (11GB). % 77.1% of the time SIDD
Table \ref{tableNINDXT1} shows denoising performance on the Fujifilm X-T1 test pictures. We observe that the model trained on the \ac{NIND} significantly outperforms \ac{BM3D} and that adding training data from the Canon EOS 500D sensor, as well as part of the \ac{SIDD} dataset, does not appear to negatively impact performance. Table \ref{tableNINDXT1onBoboC500D} and Figure \ref{fig:boboc500d} show denoising performance on the scene ``MuseeL-Bobo-C500D", where a model trained only with \ac{NIND}:\ac{X-T1} data performs nearly as well as a model that was also trained with \ac{NIND}:\ac{C500D} data (and so does a model trained with both \ac{NIND} and \ac{SIDD}). Table \ref{tableXT1onC500D} summarizes the average performance of \ac{X-T1}-trained models on ten \ac{C500D} scenes and shows performance which considerably exceeds that of \ac{BM3D} even though the model was generalizing for a different sensor type.

A model trained with only \ac{NIND}:\ac{X-T1} ISO6400 noisy images yields slightly better performance at and around ISO6400, but this comes with a considerable loss of detail at low ISO and the denoising performance becomes poor as the noise level increases. Moreover the model trained on Fujifilm X-T1 ISO6400 images appears not to generalize as well to different sensors as we found it consistently performs worse on the Canon 500D images. These findings suggest that the cost of generalization is acceptably low and therefore a model mostly benefits from learning with different noise levels and sensors.

Reconstructing the noise (Method \ref{exp:nindmakenoise}) as was suggested in \cite{dncnn} typically yields performance below that of BM3D when applied to ISO noise. The difficulty in reconstructing ISO noise was further noticed in an experiment where we mistakenly fed our learning model noisy images as ground-truth 31\% of the time and it still exceeded BM3D performance. This went as far as inverting the clean and noisy crops and still learning an appreciable level of denoising. These findings suggest that a model can easily tolerate some noise in the ground-truth data. Research into this topic \cite{noise2noise}  has been performed to explicitly train models on noisy data and rely on the zero-mean nature of the noise to effectively remove various artificial noise distributions.

Training a model using artificial noise added to ground-truth images (Method \ref{exp:nindart}), as is commonplace in the literature \cite{rednet}\cite{dncnn}, yields the worst performance in our tests.

Figure \ref{fig:text} shows satisfying denoising performance of a model trained on \ac{X-T1} images with text recovered from a \ac{C500D} image. In addition to the aforementioned results based on \ac{SSIM}, we have subjectively tested our \ac{NIND}-trained model on single images which are not part of the dataset. The first such image is that of a dynamic outdoor scene in which a human walks towards a group of pigeons, causing them to disperse in multiple directions. This type of fast, moving scene cannot be included in the dataset due to its dynamic nature and it must be captured with settings that result in a poor quality image; a small aperture (f/11) to focus everywhere, a fast shutter speed (1/1500 s) to capture the flying birds, and a maximum sensor sensitivity (ISO6400) to match the aforementioned settings. Nonetheless, we found the denoised image to be of high quality; we submitted it to the Wikimedia Commons ``Quality Images Candidates" page \cite{qic} and it was subsequently promoted to a ``Quality Image" by Wikimedia Commons reviewers. A crop of the image is provided on Figure \ref{fig:visualpigeons} with a comparison between the noisy version, one denoised with a U-Net model trained on \ac{NIND}, and a version that has been denoised using \ac{BM3D} (with $\sigma=30$ which, on average, yields the highest \ac{SSIM} in our ISO6400 test images). The BM3D version shows significant displeasing artifacts, for example on the skirt and the blue uniform panel on the right, while the model trained on \ac{NIND} smoothed these regions appropriately while retaining a greater level of useful details such as those present on the pigeons' wings.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{GAN Denoising}\label{sec:GAN Denoising}

We initially trained denoising networks using the pix2pix approach (the typical use-case is generating photorealistic images from rough sketches) of performing image to image translation using a PatchGAN discriminator and a U-Net generator \cite{pix2pix}. The results were not satisfying even after applying the adjustments mentioned in section \ref{sec:Initial training method}; the PatchGAN would consistently prefer high frequencies and the denoising performance suffered. We thus designed our own HulDisc discriminator and a similar HulbNet generator and tested their individual building blocks in section \ref{sec:Components} before assessing the system as a whole and comparing it to previous approaches in section \ref{sec:System as a whole}.

\subsection{PatchGAN}

We first tried following the pix2pix approach using a PatchGAN discriminator and a U-Net generator \cite{pix2pix} with 1:3 training ratio. The discriminator handles high-frequency details and low-frequencies are handled by the $\ell 1$ loss. We were unable to obtain satisfying performance after experimenting with different loss balances, we found that the denoising performance was subpar to that of \ac{BM3D} and of our previous U-Net network; high-frequencies tend to be exaggerated in low-frequency areas and high frequency details are not properly recovered, often resulting in an overall poor \ac{SSIM} index. Figure \ref{fig:p2p-def} illustrates the denoising performance obtained using a 0.25 $\ell 1$ multiplier.

\begin{figure}[!htbp]
  \begin{center}
    \includegraphics[width=1\linewidth]{gfx/comp/p2p_def.jpg}
    \caption[Initial (c)GAN denoising (visual comparison)]{Comparison of standard pix2pix \cite{pix2pix} applied to image denoising with other methods on a crop from image NIND\_MuseeL-Bobo\_ISOH2. 1: ISO200 ground-truth crop (SSIM: 1.00), 2: ISOH2 noisy crop (SSIM: 0.151), 3: Denoised image with a single U-Net network trained on the \ac{NIND} (SSIM: 0.810), 4: Denoised image with a standard pix2pix network trained on the \ac{NIND} with $0.25\times\ell 1$ (SSIM: 0.672), 5: Denoised image with a pix2pix network trained on the \ac{NIND} with the following adjustments: minimum of 35 iterations with $\text{loss}_\text{SSIM} < 0.125$ before using the discriminator with $\text{loss}_G=0.5 \text{loss}_D + 0.4 \text{loss}_\text{SSIM} + 0.1 \text{loss}_{\ell 1}$ (SSIM: 0.794),6: Denoised with \ac{BM3D} using $\sigma=99$ (SSIM: 0.694)}
    \label{fig:p2p-def}
  \end{center}
\end{figure}

Some improvement were made with careful fine-tuning of the learning environment. We added the \ac{SSIM} loss used in single-network training and tested different balances between the $\ell 1$, \ac{SSIM}, and discriminative losses. We tried training the generator without applying a discriminative loss until a SSIM threshold is met for a given number of consecutive iterations, and we tried training a non conditional network. Figure \ref{fig:p2p-comp-1} shows performance on the "stefantiek" set using different settings on a non conditional network (as well as the baseline network performance without these adjustments). We observe particularly poor performance when the discriminative loss has a high weight (weight\_D = 0.9), as well as when we start using the discriminative loss early on before the generator has learned to properly denoise with the conventional loss functions (min\_SSIM=0.3). The GAN network obtains reasonable performance when the generator is pretrained to achieve a \ac{SSIM} loss of 0.125 to 0.15. The greatest performance shown occurs when the generator has to obtain $\text{loss}_\text{SSIM} < 0.125$ for 35 consecutive iterations before using the discriminator as half of the overall loss. A crop denoised with this model (5) is shown beside one trained with standard pix2pix \ac{cGAN} (4) on Figure \ref{fig:p2p-def}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{gfx/graphs/p2p-comp-1.pdf}
\caption[Denoising performance of \acsp{GAN} on the "stefantiek" set]{Denoising performance of the non conditional GAN with varying weights and schedules on the "stefantiek" set over increased ISO values. The models appear to perform better when they are pre-trained to achieve good performance with the \acs{SSIM} loss before taking the discriminator's feedback into account, and when the discriminator's weight (1 - other weights) does not account for the bulk of the loss function.}
\label{fig:p2p-comp-1}
\end{figure}

\subsection{Our proposed architecture}
\subsubsection{Components}\label{sec:Components}
Our main motivation for designing the "Hul" architectures described in sections \ref{HulbNet} and \ref{HulDisc} is the poor performance obtained with a PatchGAN discriminator; the discriminative network should not focus only on high-frequencies or its feedback becomes corruptive. We also wanted to replace the U-Net generative network which loses too much border that is either easily discriminated against or that does not leave a large enough crop for the discriminator to make up meaningful results. We thus designed both a generator and a discriminator which follow the same design ideas (alas the discriminator is cut in half and thus loses many of the dense connections), and tested specific components for their respective use-cases.
%Network like \cite{pix2pixhd} which processes multiple scales to capture high-frequencies and smooth textures. Instead of dividing up the images into different resolutions or using multiple networks, we followed an approach similar to that of DenseNet \cite{densenet} which concatenates multiple channels together from different layers (as opposed to summing the features \cite{resnet}), allowing layers to contain different types of features captured with different convolutions. Some of the convolutions thus capture a wider receptive field using dilation which has been shown to perform well in image segmentation tasks \cite{multi-scale-dilation}, while other channels focus on local details with standard convolutions.

%At the same time we wanted to replace the U-Net network which loses too much border that is either easily discriminated against or does not leave a large enough crop for the discriminator to make up meaningful results. We thus designed both a generator and a discriminator which follow the same design ideas (alas the discriminator is cut in half and loses many of the dense connections), and tested specific components for their respective use-cases.

The two architectures were trained separately to assess and optimize their individual performance before working together. The generator is typically trained using the same framework as that of section \ref{sec:CNN Denoising}, while the discriminator main tests involve discriminating between clean and noisy crops from the dataset without receiving the conditional input. This test is not as robust as training with the result of a static generator but it allows us to quickly get a preview of the discriminator's performance whereas it would not always be able to learn from scratch given a well trained generator.

\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{gfx/graphs/gen-BN.pdf}
\caption[Generative network learning curve with and without batch normalization]{Generative network learning curve with and without \acl{BN} ("HulbNet"), as well as with neither \ac{BN} nor an activation function. We observe better performance when \acl{BN} is not applied and with the use of a non-linear final activation function.}
\label{fig:gen-BN}
\end{figure}

\acl{BN} appeared to hamper learning on the generative networks, often yielding worse color rendition and lowering the \ac{SSIM} index (which does not appear to be very sensible to color accuracy). We trained a generative network for an epoch with and without batch normalization to finally determine whether \ac{BN} should be removed from the generative network. Figure \ref{fig:gen-BN} shows the resulting learning curve, which allows us to confidently remove \ac{BN} from the generative architecture (we add the letter b such that "HulbNet" denotes a network without \ac{BN}). We also trained a network for an epoch to determine whether not using a final loss function as is sometimes done \cite{dncnn} would yield better performance than \ac{PReLU}, but we found the network performs better with a non-linear activation function.

% TODO fix imbalance
% TODO explicitly mention loss
\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{gfx/graphs/hul112disc-activ.pdf}
\caption[Discriminative network learning curve with different final activation functions and number of filters]{Discriminative network learning curve with varying final activation functions (FA) and number of filters. We observe that the discriminative network benefits greatly from \ac{BN} and that training stability and performance suffer from not having a sufficient number of filters. One funit denotes the number of filters multiplier (with the first layer outputting two funits). The network is not conditioned and its task is to discriminate whether an image is clean or noisy. The loss is squared therefore a \ac{MSE} of 0.25 is equivalent to guessing 0.5.}
\label{fig:hul112disc-activ}
\end{figure}

% TODO add hulfdisc
Figure \ref{fig:hul112disc-activ} shows the discriminator's learning curve classifying images as either clean or noisy. We observe that \acl{BN} is useful in the case of the discriminator, likely because its job does not entail recreating image fidelity from the input but output a single probability, and \acl{BN} can help when given mini-batches of a single target value (line 3, 6, 7). We observe that the \ac{PReLU} activation function yields sporadically very poor performance on line 0 and 4, but performance is restored with a greater number of filters. Ending the discriminative network with a final max-pooling appears worse than a learned convolution. The different activation functions seem roughly equivalent as they overtake one other over time. Although we observe the importance of using a sufficient number of filters, we cannot conclude an actual number of needed filters from this experiment because the task is simpler than discriminating between real (baseline) and fake (denoised) images, but the real task at hand cannot be accomplished because the discriminator never catches up if there is a large discrepancy between an untrained discriminator and a pre-trained generator.

\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{gfx/graphs/label-smoothing.pdf}
\caption[Discriminative network performance with noisy labels (always noisy or noisy for positive labels only)]{Discriminative network performance (\ac{MSE} loss) with noisy labels (either always noisy or noisy for positive labels only). The network attempts to discriminate images denoised by a generator which has been trained for three epochs and is continuously being updated (alas without any input from the discriminator). We did not observe a notable difference in performance between the two label smoothing methods.}
\label{fig:label-smoothing}
\end{figure}

Following some incertitude on whether label smoothing should only be applied on positive labels as per \cite{gantechniques} or on all batches used to train the discriminator, we compared the two approaches and show the results on figure \ref{fig:label-smoothing}. The test protocol is different from the previous one in that the discriminator attempts to discriminate against images denoised by a generator which has been pre-trained for three epochs (and is continuously being updated, although it receives no feedback from the discriminator). The results we observed did not seem to differ significantly as the two discriminators would continuously overtake one other.

% switching losses is bad
\subsubsection{System as a whole}\label{sec:System as a whole}

In this section we test different configurations of our "Hul" \ac{cGAN} networks as a whole system trained according to our trained method described in section \ref{sec:Adopted training method}. The performance is compared to the previously used configurations as well (U-Net alone and the PatchGAN), and we assess the visual quality of denoised images where it was previously lacking (i.e. noisy face images).

The different types of network present in this experiment are as follow:
\begin{itemize}
  \item Hul is the base name of the network we use
  \item b: no batch normalization
  \item f: more channels as the other dimensions decrease
  \item A number representing the minimum valid input size
  \item Disc for discriminators or Net for generators
\end{itemize}

\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth]{gfx/graphs/cgan-comp.pdf}
\caption[cGAN learning curve]{
cGAN training performance comparison. 0: The U-Net model from experiment \ref{sec:CNN Denoiser}:\ref{exp:nindxt1}, 1: Hulb128Net (without a discriminator). The following networks all use the Hulb128Net generator and a specified discriminator. 2: Hul112Disc, 3: Hulf112Disc (more filters as the network shrinks), 4: PatchGAN \cite{pix2pix}, 5: Hul112Disc which learns 10 \% more often than the generator, 6: Hulf112Disc which learns 10 \% more often than the generator.
Our HulbNet generator appears to outperform U-Net both as a single network and in a \acs{GAN}. Although the \acs{cGAN} system appears to perform better, that is expected because the main loss is no longer based on a correlation with the ground-truth image, and visual analysis of the results later confirm that the \acs{cGAN} system offers more visually pleasing denoising. Adding more filters to the discriminator appears to only work if the discriminator has a higher training ratio to optimize them. Likewise, a discriminator that is trained more often does not appear to offer a better feedback if its capacity is not high enough.}
\label{fig:cgan-comp}
\end{figure}

% generate faces, graph on an image

The networks' training performance is shown on figure \ref{fig:cgan-comp}. The baseline is a single generative U-Net network, and we also trained a single Hulb128Net network. The single Hulb128Net network appears to perform better than the U-Net of our previous experiments. We observe that the increased number of filters (Hulf112Disc) destabilizes the discriminator, but performance is restored when the discriminator has more time to learn. On the other hand, giving more time to a discriminator (0.1 advantage) that does not have as much capacity provides worse feedback and the result of the generator suffers. The PatchGAN's performance seems to degenerate quickly, and the Hulb128Net alone offers greater performance than that of a \acs{cGAN} pair but further analysis shows that the \acs{cGAN} denoised images are visually superior. The networks that were trained for longer benefit greatly from the additional time and exceed the performance of a single U-Net generator which maxes out at \acs{SSIM} index = 0.902. The \acs{SSIM} training loss is shown. Although \acs{SSIM} is not a meaningful enough factor on its own because the network could recreate a structurally different yet visually pleasing image, it provides an approximate indicator of performance and learning.

All \ac{cGAN} networks use a $\text{loss}_{\ell 1}$ weight of 0.05 and a $\text{loss}_\text{SSIM}$ weight of 0.2. While it may be worth trying different ratios, we found that the networks quickly became unstable as the weight of the discriminative loss increased; a $\text{loss}_\text{discriminative}$ weight of 0.95 would typically not complete the first epoch.


\begin{figure}[!htbp]
\centering
\includegraphics[width=1\linewidth,height=0.9\textheight,keepaspectratio]{gfx/comp/NIND_CourtineDeVillersDebris_ISOH2-cgan.jpg}
\caption[Denoising CourtineDeVillersDebris with cGANs (visual comparison)]{Visual comparison of different networks' denoising performance on NIND\_CourtineDeVillersDebris\_ISOH2. 1: Ground-truth (ISO200 1/4 s, SSIM: 1.000), 2: Noisy (High ISO 1/1100 s, SSIM: 0.232), 3: HulbNet generator with no discriminator (SSIM: 0.784), 4: HulbNet generator and HulDisc discriminator (SSIM: 0.780), 5: HulbNet generator and PatchGAN discriminator (SSIM: 0.687).}
\label{fig:cgan-wall}
\end{figure}

Figure \ref{fig:cgan-wall} visually shows the denoising performance of various networks. We only displayed one combination of "Hul" generator-discriminator pair because we could not discern an appreciatable difference between any of our trained models. We observe restored high frequencies in the \ac{cGAN} network, whereas the single generator does not provide as much detail. The PatchGAN offers a very sharp image but much of the generated image looks unnatural. The SSIM index does not correlate with a visually appealing result because it favors blur over details that are different from those present on the ground-truth (one-to-many mapping). This type of image benefits greatly from a discriminative loss because of its many high-frequency details.


%TODO (in-progress): HulbNet w/ HulDisc, HulbNet w/PatchGAN, HulbNet w/HulfDisc, HulbNet alone, HulbNet w/ HulDisc with 0.1 advantage

%(mention that training with weight\_D >= 0.95 is not stable enough) 

%TODO: discriminator\_advantage

%TODO conditional vs not conditional

%TODO invert probabilities

%TODO higher - lower weights

%TODO 
